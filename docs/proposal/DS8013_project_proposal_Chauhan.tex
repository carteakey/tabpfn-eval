\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{booktabs}
\usepackage{tabu}
\usepackage{url}

%% Sets page size and margins
\usepackage[a4paper,top=0cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{apacite}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{A replication study of transformer-based TabPFN for assessing the applicability of neural-network based solutions in tabular classification.
}
\author{Kartikey Chauhan - 501259284}
\date{}

\begin{document}
\maketitle
\section*{Overview}
Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. In particular, large transformer-based models trained on massive corpora, are disrupting machine learning in many areas. However, when it comes to tabular (a.k.a. structured) data, traditional machine learning methods, such as gradient-boosted decision trees have shown superior performance over deep learning. 
Recently, TabPFN~\cite{hollmann2023tabpfn} proposes a radical change to how tabular classification is done, introducing a pre-trained Transformer that is able to perform classification without training. This project aims to replicate and do an empirical review of TabPFN's claims, while potentially exploring avenues for further scaling and modifications based on latest advancements in the Transformer space.

\section*{Objectives}

\begin{itemize}
\item Replication of the transformer model creation and training.
\item Comparative analysis of its classfication and runtime performance against traditional ML methods \textit{(LogReg, KNN)}, AutoML systems \textit{(Auto-sklearn 2.0, Autogluon)}, and boosting machines \textit{(XGBoost, CatBoost, LightGBM)}.
\item Analysis of potential model scaling and extension via hyperparameter optimization, extended training, or alternative architectures. Currently, TabPFN's scale is limited (maximum of 1000 training data points, 100 numerical features without missing values, 10 classes).
\end{itemize}
\section*{Datasets}
\begin{itemize}
\item The comparative analysis will be based on popular datasets from \href{https://openml.org/search?type=data&sort=runs&status=active}{OpenML.org}, unseen by the model's priors, and extending the evaluation scope to more than the 18 datasets used in the paper. 
\end{itemize}
\section*{Evaluation}
\begin{itemize}
\item The \textbf{Receiver Operating Characteristic Area Under the Curve} (ROC AUC) and \textbf{Mean Time} to train and predict the classes are chosen as key metrics. The ROC AUC is a comprehensive metric that is particularly useful for imbalanced datasets, where accuracy can be misleading. Additionally, the ROC AUC is not affected by changes in the class distribution. Meanwhile, mean time to train and predict assesses the model's computational efficiency, the crucial advantage of a pretrained network.
\end{itemize}

\nocite{*}
\bibliographystyle{plain}
\bibliography{refs}
\end{document}